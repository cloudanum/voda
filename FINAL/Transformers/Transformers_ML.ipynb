{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('cloudanum').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|\n",
      "+-----------------+----------------+-----------------+----------------+\n",
      "|              5.1|             3.5|              1.4|             0.2|\n",
      "|              4.9|             3.0|              1.4|             0.2|\n",
      "|              4.7|             3.2|              1.3|             0.2|\n",
      "|              4.6|             3.1|              1.5|             0.2|\n",
      "|              5.0|             3.6|              1.4|             0.2|\n",
      "|              5.4|             3.9|              1.7|             0.4|\n",
      "|              4.6|             3.4|              1.4|             0.3|\n",
      "|              5.0|             3.4|              1.5|             0.2|\n",
      "|              4.4|             2.9|              1.4|             0.2|\n",
      "|              4.9|             3.1|              1.5|             0.1|\n",
      "|              5.4|             3.7|              1.5|             0.2|\n",
      "|              4.8|             3.4|              1.6|             0.2|\n",
      "|              4.8|             3.0|              1.4|             0.1|\n",
      "|              4.3|             3.0|              1.1|             0.1|\n",
      "|              5.8|             4.0|              1.2|             0.2|\n",
      "|              5.7|             4.4|              1.5|             0.4|\n",
      "|              5.4|             3.9|              1.3|             0.4|\n",
      "|              5.1|             3.5|              1.4|             0.3|\n",
      "|              5.7|             3.8|              1.7|             0.3|\n",
      "|              5.1|             3.8|              1.5|             0.3|\n",
      "+-----------------+----------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "iris= spark.createDataFrame(pd.read_csv(\"https://storage.googleapis.com/neurals/data/iris.csv\",header='infer'))\n",
    "dataframe = iris.drop(\"species\")\n",
    "dataframe.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assembled to vector column 'features'\n",
      "+-----+-------------+\n",
      "|label|     features|\n",
      "+-----+-------------+\n",
      "|  0.2|[5.1,3.5,1.4]|\n",
      "|  0.2|[4.9,3.0,1.4]|\n",
      "|  0.2|[4.7,3.2,1.3]|\n",
      "|  0.2|[4.6,3.1,1.5]|\n",
      "|  0.2|[5.0,3.6,1.4]|\n",
      "|  0.4|[5.4,3.9,1.7]|\n",
      "|  0.3|[4.6,3.4,1.4]|\n",
      "|  0.2|[5.0,3.4,1.5]|\n",
      "|  0.2|[4.4,2.9,1.4]|\n",
      "|  0.1|[4.9,3.1,1.5]|\n",
      "|  0.2|[5.4,3.7,1.5]|\n",
      "|  0.2|[4.8,3.4,1.6]|\n",
      "|  0.1|[4.8,3.0,1.4]|\n",
      "|  0.1|[4.3,3.0,1.1]|\n",
      "|  0.2|[5.8,4.0,1.2]|\n",
      "|  0.4|[5.7,4.4,1.5]|\n",
      "|  0.4|[5.4,3.9,1.3]|\n",
      "|  0.3|[5.1,3.5,1.4]|\n",
      "|  0.3|[5.7,3.8,1.7]|\n",
      "|  0.3|[5.1,3.8,1.5]|\n",
      "+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataframe)\n",
    "print(\" Assembled to vector column 'features'\")\n",
    "\n",
    "dataframe = output.drop(\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\")\n",
    "dataframe = dataframe.withColumnRenamed(\"petal width (cm)\",\"label\")\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "dataset = dataframe\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------+\n",
      "|         prediction|label|     features|\n",
      "+-------------------+-----+-------------+\n",
      "|0.24400000000000002|  0.1|[4.8,3.0,1.4]|\n",
      "|0.24400000000000002|  0.1|[4.9,3.1,1.5]|\n",
      "|0.24400000000000002|  0.2|[5.0,3.4,1.5]|\n",
      "|0.24400000000000002|  0.1|[4.9,3.1,1.5]|\n",
      "|0.24400000000000002|  0.2|[4.6,3.6,1.0]|\n",
      "+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.235771\n",
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_3fb83bb97ab3, depth=5, numNodes=47, numFeatures=3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = dataframe\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------+\n",
      "|         prediction|label|     features|\n",
      "+-------------------+-----+-------------+\n",
      "|0.23551936213081723|  0.1|[4.3,3.0,1.1]|\n",
      "|0.26036261609907124|  0.1|[4.9,3.1,1.5]|\n",
      "|0.21333880657526177|  0.2|[4.6,3.1,1.5]|\n",
      "|0.22635269546415066|  0.2|[4.7,3.2,1.3]|\n",
      "|0.24252675751321262|  0.3|[5.1,3.5,1.4]|\n",
      "+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.248977\n",
      "RandomForestRegressionModel: uid=RandomForestRegressor_be59b55fbced, numTrees=20, numFeatures=3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = dataframe\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------+\n",
      "|         prediction|label|     features|\n",
      "+-------------------+-----+-------------+\n",
      "| 0.2039719805861526|  0.2|[4.4,2.9,1.4]|\n",
      "| 0.1039719805861526|  0.2|[4.6,3.1,1.5]|\n",
      "| 0.2574249658997069|  0.2|[5.1,3.5,1.4]|\n",
      "|0.29861474172351005|  0.2|[5.4,3.7,1.5]|\n",
      "| 0.3498147417235101|  0.4|[5.4,3.9,1.7]|\n",
      "+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.208903\n",
      "GBTRegressionModel: uid=GBTRegressor_0dbc40912259, numTrees=10, numFeatures=3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = dataframe\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "Access the dataset at \n",
    "https://storage.googleapis.com/neurals/data/weather.csv\n",
    "\n",
    "    Select MinTemp, MaxTemp, RainFall and Temp3pm as features\n",
    "    Select Temp9am as Label\n",
    "    \n",
    "Prepare the data and train a model that can predict the label.\n",
    "What is the accuracy of your approach?\n",
    "How you can make it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
