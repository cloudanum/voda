{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Transformations\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('cloudanum').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|\n",
      "+-----------------+----------------+-----------------+----------------+\n",
      "|              5.1|             3.5|              1.4|             0.2|\n",
      "|              4.9|             3.0|              1.4|             0.2|\n",
      "|              4.7|             3.2|              1.3|             0.2|\n",
      "|              4.6|             3.1|              1.5|             0.2|\n",
      "|              5.0|             3.6|              1.4|             0.2|\n",
      "|              5.4|             3.9|              1.7|             0.4|\n",
      "|              4.6|             3.4|              1.4|             0.3|\n",
      "|              5.0|             3.4|              1.5|             0.2|\n",
      "|              4.4|             2.9|              1.4|             0.2|\n",
      "|              4.9|             3.1|              1.5|             0.1|\n",
      "|              5.4|             3.7|              1.5|             0.2|\n",
      "|              4.8|             3.4|              1.6|             0.2|\n",
      "|              4.8|             3.0|              1.4|             0.1|\n",
      "|              4.3|             3.0|              1.1|             0.1|\n",
      "|              5.8|             4.0|              1.2|             0.2|\n",
      "|              5.7|             4.4|              1.5|             0.4|\n",
      "|              5.4|             3.9|              1.3|             0.4|\n",
      "|              5.1|             3.5|              1.4|             0.3|\n",
      "|              5.7|             3.8|              1.7|             0.3|\n",
      "|              5.1|             3.8|              1.5|             0.3|\n",
      "+-----------------+----------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "iris= spark.createDataFrame(pd.read_csv(\"https://storage.googleapis.com/neurals/data/iris.csv\",header='infer'))\n",
    "dataframe = iris.drop(\"species\")\n",
    "dataframe.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assembled to vector column 'features'\n",
      "+-----+-------------+\n",
      "|label|     features|\n",
      "+-----+-------------+\n",
      "|  0.2|[5.1,3.5,1.4]|\n",
      "|  0.2|[4.9,3.0,1.4]|\n",
      "|  0.2|[4.7,3.2,1.3]|\n",
      "|  0.2|[4.6,3.1,1.5]|\n",
      "|  0.2|[5.0,3.6,1.4]|\n",
      "|  0.4|[5.4,3.9,1.7]|\n",
      "|  0.3|[4.6,3.4,1.4]|\n",
      "|  0.2|[5.0,3.4,1.5]|\n",
      "|  0.2|[4.4,2.9,1.4]|\n",
      "|  0.1|[4.9,3.1,1.5]|\n",
      "|  0.2|[5.4,3.7,1.5]|\n",
      "|  0.2|[4.8,3.4,1.6]|\n",
      "|  0.1|[4.8,3.0,1.4]|\n",
      "|  0.1|[4.3,3.0,1.1]|\n",
      "|  0.2|[5.8,4.0,1.2]|\n",
      "|  0.4|[5.7,4.4,1.5]|\n",
      "|  0.4|[5.4,3.9,1.3]|\n",
      "|  0.3|[5.1,3.5,1.4]|\n",
      "|  0.3|[5.7,3.8,1.7]|\n",
      "|  0.3|[5.1,3.8,1.5]|\n",
      "+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataframe)\n",
    "print(\" Assembled to vector column 'features'\")\n",
    "\n",
    "dataframe = output.drop(\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\")\n",
    "dataframe = dataframe.withColumnRenamed(\"petal width (cm)\",\"label\")\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer\n",
    "Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. (p=2 by default.) This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+-----+-------------+--------------------+\n",
      "|label|     features|        normFeatures|\n",
      "+-----+-------------+--------------------+\n",
      "|  0.2|[5.1,3.5,1.4]|[0.51,0.35,0.1399...|\n",
      "|  0.2|[4.9,3.0,1.4]|[0.52688172043010...|\n",
      "|  0.2|[4.7,3.2,1.3]|[0.51086956521739...|\n",
      "|  0.2|[4.6,3.1,1.5]|[0.5,0.3369565217...|\n",
      "|  0.2|[5.0,3.6,1.4]|[0.5,0.36,0.13999...|\n",
      "|  0.4|[5.4,3.9,1.7]|[0.49090909090909...|\n",
      "|  0.3|[4.6,3.4,1.4]|[0.48936170212765...|\n",
      "|  0.2|[5.0,3.4,1.5]|[0.50505050505050...|\n",
      "|  0.2|[4.4,2.9,1.4]|[0.50574712643678...|\n",
      "|  0.1|[4.9,3.1,1.5]|[0.51578947368421...|\n",
      "|  0.2|[5.4,3.7,1.5]|[0.50943396226415...|\n",
      "|  0.2|[4.8,3.4,1.6]|[0.48979591836734...|\n",
      "|  0.1|[4.8,3.0,1.4]|[0.52173913043478...|\n",
      "|  0.1|[4.3,3.0,1.1]|[0.51190476190476...|\n",
      "|  0.2|[5.8,4.0,1.2]|[0.52727272727272...|\n",
      "|  0.4|[5.7,4.4,1.5]|[0.49137931034482...|\n",
      "|  0.4|[5.4,3.9,1.3]|[0.50943396226415...|\n",
      "|  0.3|[5.1,3.5,1.4]|[0.51,0.35,0.1399...|\n",
      "|  0.3|[5.7,3.8,1.7]|[0.50892857142857...|\n",
      "|  0.3|[5.1,3.8,1.5]|[0.49038461538461...|\n",
      "+-----+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Normalized using L^inf norm\n",
      "+-----+-------------+--------------------+\n",
      "|label|     features|        normFeatures|\n",
      "+-----+-------------+--------------------+\n",
      "|  0.2|[5.1,3.5,1.4]|[1.0,0.6862745098...|\n",
      "|  0.2|[4.9,3.0,1.4]|[1.0,0.6122448979...|\n",
      "|  0.2|[4.7,3.2,1.3]|[1.0,0.6808510638...|\n",
      "|  0.2|[4.6,3.1,1.5]|[1.0,0.6739130434...|\n",
      "|  0.2|[5.0,3.6,1.4]|[1.0,0.72,0.27999...|\n",
      "|  0.4|[5.4,3.9,1.7]|[1.0,0.7222222222...|\n",
      "|  0.3|[4.6,3.4,1.4]|[1.0,0.7391304347...|\n",
      "|  0.2|[5.0,3.4,1.5]|[1.0,0.6799999999...|\n",
      "|  0.2|[4.4,2.9,1.4]|[1.0,0.6590909090...|\n",
      "|  0.1|[4.9,3.1,1.5]|[1.0,0.6326530612...|\n",
      "|  0.2|[5.4,3.7,1.5]|[1.0,0.6851851851...|\n",
      "|  0.2|[4.8,3.4,1.6]|[1.0,0.7083333333...|\n",
      "|  0.1|[4.8,3.0,1.4]|[1.0,0.625,0.2916...|\n",
      "|  0.1|[4.3,3.0,1.1]|[1.0,0.6976744186...|\n",
      "|  0.2|[5.8,4.0,1.2]|[1.0,0.6896551724...|\n",
      "|  0.4|[5.7,4.4,1.5]|[1.0,0.7719298245...|\n",
      "|  0.4|[5.4,3.9,1.3]|[1.0,0.7222222222...|\n",
      "|  0.3|[5.1,3.5,1.4]|[1.0,0.6862745098...|\n",
      "|  0.3|[5.7,3.8,1.7]|[1.0,0.6666666666...|\n",
      "|  0.3|[5.1,3.8,1.5]|[1.0,0.7450980392...|\n",
      "+-----+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataframe)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()\n",
    "\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(dataframe, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalized using L^inf norm\")\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    "StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "\n",
    "withStd: True by default. Scales the data to unit standard deviation.\n",
    "withMean: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input.\n",
    "StandardScaler is an Estimator which can be fit on a dataset to produce a StandardScalerModel; this amounts to computing summary statistics. The model can then transform a Vector column in a dataset to have unit standard deviation and/or zero mean features.\n",
    "\n",
    "Note that if the standard deviation of a feature is zero, it will return default 0.0 value in the Vector for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+\n",
      "|label|     features|      scaledFeatures|\n",
      "+-----+-------------+--------------------+\n",
      "|  0.2|[5.1,3.5,1.4]|[6.15892840883879...|\n",
      "|  0.2|[4.9,3.0,1.4]|[5.91740180457060...|\n",
      "|  0.2|[4.7,3.2,1.3]|[5.67587520030241...|\n",
      "|  0.2|[4.6,3.1,1.5]|[5.55511189816832...|\n",
      "|  0.2|[5.0,3.6,1.4]|[6.03816510670469...|\n",
      "|  0.4|[5.4,3.9,1.7]|[6.52121831524107...|\n",
      "|  0.3|[4.6,3.4,1.4]|[5.55511189816832...|\n",
      "|  0.2|[5.0,3.4,1.5]|[6.03816510670469...|\n",
      "|  0.2|[4.4,2.9,1.4]|[5.31358529390013...|\n",
      "|  0.1|[4.9,3.1,1.5]|[5.91740180457060...|\n",
      "|  0.2|[5.4,3.7,1.5]|[6.52121831524107...|\n",
      "|  0.2|[4.8,3.4,1.6]|[5.79663850243651...|\n",
      "|  0.1|[4.8,3.0,1.4]|[5.79663850243651...|\n",
      "|  0.1|[4.3,3.0,1.1]|[5.19282199176604...|\n",
      "|  0.2|[5.8,4.0,1.2]|[7.00427152377745...|\n",
      "|  0.4|[5.7,4.4,1.5]|[6.88350822164335...|\n",
      "|  0.4|[5.4,3.9,1.3]|[6.52121831524107...|\n",
      "|  0.3|[5.1,3.5,1.4]|[6.15892840883879...|\n",
      "|  0.3|[5.7,3.8,1.7]|[6.88350822164335...|\n",
      "|  0.3|[5.1,3.8,1.5]|[6.15892840883879...|\n",
      "+-----+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "dataFrame = dataframe\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizer\n",
    "Bucketizer transforms a column of continuous features to a column of feature buckets, where the buckets are specified by users. It takes a parameter:\n",
    "\n",
    "    splits: Parameter for mapping continuous features into buckets. With n+1 splits, there are n buckets. A bucket defined by splits x,y holds values in the range [x,y) except the last bucket, which also includes y. Splits should be strictly increasing. Values at -inf, inf must be explicitly provided to cover all Double values; Otherwise, values outside the splits specified will be treated as errors. Two examples of splits are Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity) and Array(0.0, 1.0, 2.0).\n",
    "    \n",
    "Note that if you have no idea of the upper and lower bounds of the targeted column, you should add Double.NegativeInfinity and Double.PositiveInfinity as the bounds of your splits to prevent a potential out of Bucketizer bounds exception.\n",
    "\n",
    "Note also that the splits that you provided have to be in strictly increasing order, i.e. s0 < s1 < s2 < ... < sn.\n",
    "\n",
    "More details can be found in the API docs for Bucketizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 6 buckets\n",
      "+-----------------+----------------+-----------------+----------------+-----------+----------------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|    species|bucketedFeatures|\n",
      "+-----------------+----------------+-----------------+----------------+-----------+----------------+\n",
      "|              5.1|             3.5|              1.4|             0.2|Iris-setosa|             5.0|\n",
      "|              4.9|             3.0|              1.4|             0.2|Iris-setosa|             4.0|\n",
      "|              4.7|             3.2|              1.3|             0.2|Iris-setosa|             4.0|\n",
      "|              4.6|             3.1|              1.5|             0.2|Iris-setosa|             4.0|\n",
      "|              5.0|             3.6|              1.4|             0.2|Iris-setosa|             5.0|\n",
      "|              5.4|             3.9|              1.7|             0.4|Iris-setosa|             5.0|\n",
      "|              4.6|             3.4|              1.4|             0.3|Iris-setosa|             4.0|\n",
      "|              5.0|             3.4|              1.5|             0.2|Iris-setosa|             5.0|\n",
      "|              4.4|             2.9|              1.4|             0.2|Iris-setosa|             4.0|\n",
      "|              4.9|             3.1|              1.5|             0.1|Iris-setosa|             4.0|\n",
      "|              5.4|             3.7|              1.5|             0.2|Iris-setosa|             5.0|\n",
      "|              4.8|             3.4|              1.6|             0.2|Iris-setosa|             4.0|\n",
      "|              4.8|             3.0|              1.4|             0.1|Iris-setosa|             4.0|\n",
      "|              4.3|             3.0|              1.1|             0.1|Iris-setosa|             4.0|\n",
      "|              5.8|             4.0|              1.2|             0.2|Iris-setosa|             5.0|\n",
      "|              5.7|             4.4|              1.5|             0.4|Iris-setosa|             5.0|\n",
      "|              5.4|             3.9|              1.3|             0.4|Iris-setosa|             5.0|\n",
      "|              5.1|             3.5|              1.4|             0.3|Iris-setosa|             5.0|\n",
      "|              5.7|             3.8|              1.7|             0.3|Iris-setosa|             5.0|\n",
      "|              5.1|             3.8|              1.5|             0.3|Iris-setosa|             5.0|\n",
      "+-----------------+----------------+-----------------+----------------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [0.0, 1.0, 2.0, 3.0, 4.0,5.0,10]\n",
    "\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"sepal length (cm)\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(iris)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer\n",
    "The Imputer estimator completes missing values in a dataset, using the mean, median or mode of the columns in which the missing values are located. The input columns should be of numeric type. Currently Imputer does not support categorical features and possibly creates incorrect values for columns containing categorical features. Imputer can impute custom values other than ‘NaN’ by .setMissingValue(custom_value). For example, .setMissingValue(0) will impute all occurrences of (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, float(\"nan\")),\n",
    "    (2.0, float(\"nan\")),\n",
    "    (float(\"nan\"), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], [\"a\", \"b\"])\n",
    "\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "Apply the following transformations on the dataframe below:\n",
    "\n",
    "1- Vector Assembler\n",
    "\n",
    "2- Normalizer\n",
    "\n",
    "3- Standard Scaler\n",
    "\n",
    "4- Bucketizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, Vectors.dense([1.7, 4.4, 7.6, 5.8, 9.6, 2.3]), 3.0,),\n",
    "    (2, Vectors.dense([8.8, 7.3, 5.7, 7.3, 2.2, 4.1]), 2.0,),\n",
    "    (3, Vectors.dense([1.2, 9.5, 2.5, 3.1, 8.7, 2.5]), 3.0,),\n",
    "    (4, Vectors.dense([3.7, 9.2, 6.1, 4.1, 7.5, 3.8]), 2.0,),\n",
    "    (5, Vectors.dense([8.9, 5.2, 7.8, 8.3, 5.2, 3.0]), 4.0,),\n",
    "    (6, Vectors.dense([7.9, 8.5, 9.2, 4.0, 9.4, 2.1]), 4.0,)], [\"id\", \"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
